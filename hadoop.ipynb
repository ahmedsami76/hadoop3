{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scalable File System that handles the failure of nodes without data and can scale up horizontally to any number of nodes. The initial goal of HDFS was to serve large data files with high read and write performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fault tolerance**\n",
    "- **Streaming data access**: HDFS works on a write once read many principle. HDFS does not wait for the entire file to be read before sending data to the client; instead, it sends data as soon as it reads it. The client can immediately process the received stream, which makes data processing efficient.\n",
    "- **Scalability**: Storing a huge number of small files is generally not recommended; the size of the file should be equal to or greater than the block size. Small files consume memory from name node.\n",
    "- **Simplicity**\n",
    "- **High availability**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slow porcessing of large datasets on one computer\n",
    "    - Isolate compute and data\n",
    "    - Simultaneous processing on multiple chunks of data\n",
    "- Movement of large datasets between data nodes\n",
    "    - data replication on multiple nodes\n",
    "    - mode compute closer to data node\n",
    "- Mutliple access randomly by many users modifying files mights cause inconsistencies\n",
    "    - Only append and truncation allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hdfsarchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "- Data Plane:\n",
    "    - Data blocks\n",
    "    - Replication\n",
    "    - Checkpoints\n",
    "    - File metadata\n",
    "- Control Plane: \n",
    "    - NameNodes\n",
    "    - DataNodes\n",
    "    - JournalNodes\n",
    "    - Zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control Plane**  \n",
    "**NameNode**  \n",
    "- All data operations will first go through a NameNode and then to other relevant Hadoop components. \n",
    "- The NameNode manages the File System namespace. \n",
    "- It stores the File System tree and metadata of files and directories namely _File System namespace_, image (_fsimage_) files, and _edit logs_ files.\n",
    "\n",
    "**DataNode**  \n",
    "- They perform data block operations (creation, modification, or deletion) based on instructions that are received from NameNodes or HDFS clients. \n",
    "- They host data processing jobs such as MapReduce. \n",
    "- They report back block information to NameNodes.\n",
    "- DataNodes also communicate between each other in the case of data replication.\n",
    "\n",
    "**JournalNode**  \n",
    "- With NameNode high availability, there was a need to manage edit logs and HDFS metadata between a active and standby NameNodes. \n",
    "- JournalNodes were introduced to efficiently share edit logs and metadata between two NameNodes.\n",
    "- JournalNodes exercise concurrency write locks to ensure that edit logs are written by one active NameNode at a time.\n",
    "\n",
    "**Zookeeper**  \n",
    "- HA without automatic failover would have manual intervention to bring NameNode services back up in the event of failure. \n",
    "- Zookeeper Quorum and Zookeeper Failover controller, also known as ZKFailoverController (ZKFC). \n",
    "- Zookeeper maintains data about NameNode health and connectivity. \n",
    "- It monitors clients and notifies other clients in the event of failure. \n",
    "- Zookeeper maintains an active persistent session with each of the NameNodes, and this session is renewed by each of them upon expiry. \n",
    "- In the event of a failure or crash, the expired session is not renewed by the failed NameNode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Plane**  \n",
    "**Replication**  \n",
    "**Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hdfscommarch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NameNode keeps the complete _fsimage_ in memory so that all the metadata information requests can be served in the smallest amount of time possible and persist fsimage and edit logs on the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# do some hdfs operations\n",
    "`hdfs dfs -mkdir /testdir`  \n",
    "`hdfs dfs -touch /testdir/somefile`  \n",
    "`hdfs dfs -copyFromLocal /etc/hosts /testdir/`  \n",
    "`hdfs dfs -cat /testdir/hosts`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to fetch the fsimage in readable format  \n",
    "`hdfs dfsadmin -fetchImage /opt/hadoop`  \n",
    "\n",
    "to read the file using Offline Image Viewer tool  \n",
    "`hdfs oiv -i /opt/hadoop/fsimage_0000000000000000000 -o /opt/hadoop/fsimage_output.csv -p Delimited`\n",
    "\n",
    "to fetch edits file  \n",
    "`hdfs oev -i /tmp/hadoop-hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -o /tmp/edits-log.xml`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
